# **Company Similarity Retrieval Service**

## **Overview**
This project implements a Python-based service to retrieve and rank companies similar to a given target company. The similarity is determined using textual descriptions and other attributes from the dataset. The service uses machine learning techniques, specifically embeddings generated by Sentence-BERT, to measure similarity and provides results via a RESTful API.

This solution is designed to be scalable, modular, and ready for integration into a production environment.

---

## **Features**
1. **Data Preprocessing**:
   - Clean and normalize textual data.
   - Handle missing values in the dataset.
2. **Embedding Generation**:
   - Use a pre-trained Sentence-BERT model to generate embeddings for company descriptions.
3. **Similarity Computation**:
   - Compute cosine similarity between company embeddings.
   - Rank companies based on similarity scores.
4. **API Development**:
   - Expose an API endpoint `/retrieve_similar_companies/{company_id}` to return the top similar companies.
5. **Testing**:
   - Unit tests for utility functions and API endpoints.
6. **Scalability**:
   - Optimized for handling large datasets with efficient embedding storage and retrieval.

---

## **Dataset**
The dataset consists of the following files:
1. `companies.csv`: Contains basic information about companies such as ID, name, description, size, location, etc.
2. `company_industries.csv`: Maps companies to their respective industries.
3. `company_specialities.csv`: Maps companies to their specialities.
4. `ground_truth.json`: Provides a list of similar companies for selected target companies as ground truth for validation.

---

## **Technologies Used**
- **Programming Language**: Python
- **Libraries**: 
  - `pandas` (data manipulation)
  - `numpy` (numerical operations)
  - `scikit-learn` (cosine similarity)
  - `sentence-transformers` (embedding generation)
  - `fastapi` (API development)
  - `pytest` (testing)
- **Pre-trained Model**: Sentence-BERT (`all-MiniLM-L6-v2`)
- **Containerization**: Docker

---

## **Setup Instructions**

### **1. Clone the Repository**
```

git clone <repository-url>
cd project

```

### **2. Install Dependencies**
Create a virtual environment and install required Python packages:
```

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

```

### **3. Run the API**
Start the FastAPI server:
```

uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

```

### **4. Access the API**
Use tools like Postman or CURL to interact with the API:
- Endpoint: `/retrieve_similar_companies/{company_id}`
- Example Request:
```

curl http://127.0.0.1:8000/retrieve_similar_companies/1009

```
- Example Response:
```

{
"company_id": 1009,
"similar_companies": [
{"id": 1016, "similarity": 0.89},
{"id": 1023, "similarity": 0.87},
{"id": 1032, "similarity": 0.85}
]
}

```

---

## **Testing Instructions**
Run unit tests using Pytest:
```

pytest tests/

```

---

## **Directory Structure**
```

project/
├── app/
│   ├── main.py          \# API implementation
│   ├── utils.py         \# Helper functions
│   ├── data_loader.py   \# Data loading and merging logic
│   ├── similarity.py    \# Embedding generation and ranking logic
│   └── __init__.py
├── data/
│   ├── companies.csv
│   ├── company_industries.csv
│   ├── company_specialities.csv
│   └── ground_truth.json
├── tests/
│   ├── test_utils.py    \# Unit tests for utilities
│   ├── test_api.py      \# Tests for API endpoints
│   └── __init__.py
├── Dockerfile           \# Docker configuration for containerization
├── requirements.txt     \# Python dependencies
└── README.md            \# Project documentation (this file)

```

---

## **Assumptions**
1. The primary feature used for similarity is the company description.
2. Missing values in descriptions are replaced with empty strings during preprocessing.
3. The service is designed to handle up to ~24,000 companies efficiently but can be scaled further with caching or distributed systems.

---

## **Future Improvements**
1. Integrate additional features like industries and specialities into the similarity computation.
2. Use a database for storing embeddings instead of in-memory storage for scalability.
3. Optimize embedding generation by precomputing embeddings during data ingestion.
